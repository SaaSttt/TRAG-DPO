import torch
from transformers import AutoTokenizer
from peft import PeftModel
from llava.model.language_model.llava_llama import LlavaLlamaForCausalLM as LlavaForConditionalGeneration

# è·¯å¾„é…ç½®
BASE_MODEL_PATH = "TRAG-DPO/llava-v1.5-7b"
LORA_PATH = "TRAG-DPO/DPO-RAG-main/train/dpo/checkpoints/llava-v1.5-7b-task-lora_dpo"
MERGED_MODEL_PATH = "TRAG-DPO/llava-v1.5-7b-dpo"

def merge_lora_weights():
    """åˆå¹¶ LoRA æƒé‡åˆ°åŸºç¡€æ¨¡å‹ï¼Œå¹¶ä¿å­˜"""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    print("âœ… åŠ è½½åŸºç¡€æ¨¡å‹...")
    base_model = LlavaForConditionalGeneration.from_pretrained(
        BASE_MODEL_PATH,
        torch_dtype=torch.float16,
        device_map="auto",
        low_cpu_mem_usage=True
    )

    print("âœ… åŠ è½½ LoRA æƒé‡...")
    model = PeftModel.from_pretrained(
        base_model,
        LORA_PATH,
        is_trainable=False,
        device_map="auto",
    )

    print("âœ… åˆå¹¶ LoRA åˆ°åŸºç¡€æ¨¡å‹...")
    model = model.merge_and_unload()
    
    print(f"âœ… ä¿å­˜åˆå¹¶æ¨¡å‹è‡³ï¼š{MERGED_MODEL_PATH}")
    model.save_pretrained(MERGED_MODEL_PATH)

    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, use_fast=False, trust_remote_code=True)
    tokenizer.save_pretrained(MERGED_MODEL_PATH)

    print("ğŸ‰ æ¨¡å‹åˆå¹¶å®Œæˆï¼Œåç»­å¯ç›´æ¥åŠ è½½åˆå¹¶åçš„æ¨¡å‹ï¼")

if __name__ == "__main__":
    merge_lora_weights()
